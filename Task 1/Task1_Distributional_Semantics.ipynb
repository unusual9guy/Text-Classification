{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "7hutnnXTwLVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all the required libraries"
      ],
      "metadata": {
        "id": "32wrqEQjlFQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from string import punctuation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l755Z6AKlKVt",
        "outputId": "ffab8156-e552-4bca-e908-508ce61bbe3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the training dataset"
      ],
      "metadata": {
        "id": "UzW4f01qlRMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('Task-1-test-dataset3.csv')"
      ],
      "metadata": {
        "id": "Y4bqq8SJlUIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing for tfidf"
      ],
      "metadata": {
        "id": "U1jsg1gPmZC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "processed_df = train_data.copy()\n",
        "\n",
        "# Tfidf\n",
        "def preprocess_text_tfidf(un_processed_data):\n",
        "    tokens = word_tokenize(un_processed_data)\n",
        "    tokenized_words = []\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    for token in tokens:\n",
        "        # Checking if the word is alphabetic\n",
        "        if token.isalpha() and token.lower() not in stop_words and token.lower() not in string.punctuation:\n",
        "            # Converting the word to lowercase\n",
        "            tokenized_words.append(lemmatizer.lemmatize(token.lower()))\n",
        "\n",
        "    preprocessed_data = ' '.join(tokenized_words)\n",
        "    return preprocessed_data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "processed_df['tokenized_synopsis_tfidf'] = processed_df['plot_synopsis'].apply(preprocess_text_tfidf)"
      ],
      "metadata": {
        "id": "oxr148qQmbQE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "5ada0a07-4f56-40c8-8117-82332fbec203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'plot_synopsis'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'plot_synopsis'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ec8df265b3c7>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mprocessed_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_synopsis_tfidf'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'plot_synopsis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'plot_synopsis'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing for word2vec"
      ],
      "metadata": {
        "id": "-FnfEfbikLfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word2Vec\n",
        "def preprocess_text_word2vec(un_processed_data):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(un_processed_data)\n",
        "\n",
        "    # Removing. punctuation\n",
        "    filtered_tokens_p = []\n",
        "    for token in tokens:\n",
        "        if token not in string.punctuation:\n",
        "            filtered_tokens_p.append(token)\n",
        "\n",
        "   # tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = []\n",
        "    for token in filtered_tokens_p:\n",
        "        lemmatized_tokens.append(lemmatizer.lemmatize(token.lower()))\n",
        "\n",
        "     # tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
        "\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    filtered_tokens_sw = []\n",
        "    for token in lemmatized_tokens:\n",
        "        if token.lower() not in stop_words:\n",
        "            filtered_tokens_sw.append(token)\n",
        "\n",
        "    # tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    return filtered_tokens_sw\n",
        "\n",
        "\n",
        "processed_df['tokenized_synopsis_word2vec'] = processed_df['plot_synopsis'].apply(preprocess_text_word2vec)"
      ],
      "metadata": {
        "id": "8sGwpSeekLpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Sparse representation (BoW with tf*idf)"
      ],
      "metadata": {
        "id": "8T95jlhnmiU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_df['tokenized_synopsis_tfidf'])\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "1n-JpURgm4Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dense static representation (Word2Vec)"
      ],
      "metadata": {
        "id": "qSaiPJ6W1M5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = Word2Vec(processed_df['tokenized_synopsis_word2vec'], vector_size=100, window=1, min_count=1, sg=0)"
      ],
      "metadata": {
        "id": "zyrkP2h61Xa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the validation dataset"
      ],
      "metadata": {
        "id": "V2VW-EnfoAaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data_task_1 = pd.read_csv('./data/Task-1-validation-dataset.csv', header=None, names=['id', 'term1', 'term2', 'value'])"
      ],
      "metadata": {
        "id": "Np1Vq3Q6oAgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the cosine similarity for tfidf for validation"
      ],
      "metadata": {
        "id": "9GhCx7658pV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# Creating a vocabulary for dealing with OOV words\n",
        "vocabulary = set(tfidf_df.columns)\n",
        "print(len(vocabulary))\n",
        "print(\"Index:\", tfidf_df.index)\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "def cosine_similarity_tfidf(term1, term2, tfidf_df):\n",
        "    # Checking if the terms are in the vocabulary or not\n",
        "    tokens_1 = term1.split()\n",
        "    tokens_2 = term2.split()\n",
        "\n",
        "    # vectors1 = [tfidf_df[token].values if token in vocabulary else np.full(tfidf_df.shape[0], 0.01) for token in tokens1]\n",
        "    vectors_1 = []\n",
        "    for token in tokens_1:\n",
        "        if token in vocabulary:\n",
        "            vectors_1.append(tfidf_df[token].values)\n",
        "        else:\n",
        "            vectors_1.append(np.full(tfidf_df.shape[0], 0.01))\n",
        "\n",
        "    # vectors2 = [tfidf_df[token].values if token in vocabulary else np.full(tfidf_df.shape[0], 0.01) for token in tokens2]\n",
        "\n",
        "    vectors_2 = []\n",
        "    for token in tokens_2:\n",
        "        if token in vocabulary:\n",
        "            vectors_2.append(tfidf_df[token].values)\n",
        "        else:\n",
        "            vectors_2.append(np.full(tfidf_df.shape[0], 0.01))\n",
        "\n",
        "    avg_vector_1 = np.mean(vectors_1, axis=0)\n",
        "    avg_vector_2 = np.mean(vectors_2, axis=0)\n",
        "\n",
        "    avg_vector_1_2d = avg_vector_1.reshape(1, -1)\n",
        "    avg_vector_2_2d = avg_vector_2.reshape(1, -1)\n",
        "\n",
        "    return cosine_similarity(avg_vector_1_2d, avg_vector_2_2d)[0][0]\n",
        "\n",
        "for id, (term1, term2) in zip(validation_data_task_1['id'], zip(validation_data_task_1['term1'], validation_data_task_1['term2'])):\n",
        "    similarity = cosine_similarity_tfidf(term1, term2, tfidf_df)\n",
        "    results.append((id, similarity))\n",
        "\n",
        "results_df = pd.DataFrame(results, columns=['id', ''])\n",
        "results_df.to_csv('10949863-Task1-method-a-validation.csv', index=False, header=False)\n",
        "files.download('10949863-Task1-method-a-validation.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D9jyq0FI8tQO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "133f7c14-c3cc-4a56-a67c-016fc67af723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79109\n",
            "Index: RangeIndex(start=0, stop=8257, step=1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0df5a4f5-d745-4b02-aec7-bd168b4db35e\", \"10949863-Task1-method-a-validation.csv\", 3365)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the test dataset"
      ],
      "metadata": {
        "id": "ksuIAvlGqVmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_task_1 = pd.read_csv('./data/Task-1-test-dataset1.csv', header=None, names=['id', 'term1', 'term2', 'value'])"
      ],
      "metadata": {
        "id": "gTul4unYqVsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the cosine similarity for tfidf for test"
      ],
      "metadata": {
        "id": "-fK8QMH_05Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# Creating a vocabulary for dealing with OOV words\n",
        "vocabulary = set(tfidf_df.columns)\n",
        "print(len(vocabulary))\n",
        "\n",
        "results = []\n",
        "\n",
        "def cosine_similarity_tfidf(term1, term2, tfidf_df):\n",
        "    # Checking if the terms are in the vocabulary or not\n",
        "    tokens_1 = term1.split()\n",
        "    tokens_2 = term2.split()\n",
        "\n",
        "    vectors_1 = []\n",
        "    for token in tokens_1:\n",
        "        if token in vocabulary:\n",
        "            vectors_1.append(tfidf_df[token].values)\n",
        "        else:\n",
        "            vectors_1.append(np.full(tfidf_df.shape[0], 0.01))\n",
        "\n",
        "\n",
        "    vectors_2 = []\n",
        "    for token in tokens_2:\n",
        "        if token in vocabulary:\n",
        "            vectors_2.append(tfidf_df[token].values)\n",
        "        else:\n",
        "            vectors_2.append(np.full(tfidf_df.shape[0], 0.01))\n",
        "\n",
        "    avg_vector_1 = np.mean(vectors_1, axis=0)\n",
        "    avg_vector_2 = np.mean(vectors_2, axis=0)\n",
        "\n",
        "    avg_vector_1_2d = avg_vector_1.reshape(1, -1)\n",
        "    avg_vector_2_2d = avg_vector_2.reshape(1, -1)\n",
        "\n",
        "    return cosine_similarity(avg_vector_1_2d, avg_vector_2_2d)[0][0]\n",
        "\n",
        "for id, (term1, term2) in zip(test_data_task_1['id'], zip(test_data_task_1['term1'], test_data_task_1['term2'])):\n",
        "    similarity = cosine_similarity_tfidf(term1, term2, tfidf_df)\n",
        "    results.append((id, similarity))\n",
        "\n",
        "# Creating a DataFrame\n",
        "results_df = pd.DataFrame(results, columns=['id', ''])\n",
        "\n",
        "results_df.to_csv('10949863-Task1-method-a-test.csv', index=False, header=False)\n",
        "files.download('10949863-Task1-method-a-test.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "9NW3mrBM05RX",
        "outputId": "64a13acd-aed9-4e73-a1a4-4f7a7d863871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_cda354a2-a145-4f53-bf36-d583bab596c1\", \"10949863-Task1-method-a-test.csv\", 2354)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the validation dataset"
      ],
      "metadata": {
        "id": "1bFE7iHRrPXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_data_task_1 = pd.read_csv('./data/Task-1-validation-dataset.csv', header=None, names=['id', 'term1', 'term2', 'value'])"
      ],
      "metadata": {
        "id": "jYOlXAotrPcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the cosine similarity for word2vec for validation"
      ],
      "metadata": {
        "id": "b6H_0EdH3-uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Creating a vocabulary for dealing with OOV words\n",
        "vocabulary = set(word2vec_model.wv.key_to_index.keys())\n",
        "\n",
        "# print(len(vocabulary))\n",
        "\n",
        "results = []\n",
        "\n",
        "# Extracting adjacent pairs of terms\n",
        "term_pairs = [(term1, term2) for term1, term2 in zip(validation_data_task_1['term1'], validation_data_task_1['term2'])]\n",
        "\n",
        "def cosine_similarity_word2vec(term1, term2, word2vec_model):\n",
        "    # Checking if the terms are in the vocabulary or not\n",
        "    tokens_1 = term1.split()\n",
        "    tokens_2 = term2.split()\n",
        "\n",
        "    vectors_1 = []\n",
        "    for token in tokens_1:\n",
        "        if token in vocabulary:\n",
        "            vectors_1.append(word2vec_model.wv.get_vector(token))\n",
        "        else:\n",
        "            vectors_1.append(np.full(word2vec_model.vector_size, 0.01))\n",
        "\n",
        "    vectors_2 = []\n",
        "    for token in tokens_2:\n",
        "        if token in vocabulary:\n",
        "            vectors_2.append(word2vec_model.wv.get_vector(token))\n",
        "        else:\n",
        "            vectors_2.append(np.full(word2vec_model.vector_size, 0.01))\n",
        "\n",
        "    avg_vector_1 = np.mean(vectors_1, axis=0)\n",
        "    avg_vector_2 = np.mean(vectors_2, axis=0)\n",
        "\n",
        "    avg_vector_1_2d = avg_vector_1.reshape(1, -1)\n",
        "    avg_vector_2_2d = avg_vector_2.reshape(1, -1)\n",
        "\n",
        "    similarity = cosine_similarity(avg_vector_1_2d, avg_vector_2_2d)[0][0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Calculate cosine similarity for each term pair\n",
        "for id, (term1, term2) in zip(range(1, len(validation_data_task_1) + 1), term_pairs):\n",
        "    similarity = cosine_similarity_word2vec(term1, term2, word2vec_model)\n",
        "    results.append((id, similarity))\n",
        "\n",
        "# Creating a DataFrame\n",
        "results_df = pd.DataFrame(results, columns=['id', 'cosine_similarity'])\n",
        "\n",
        "results_df.to_csv('10949863-Task1-method-b-validation.csv', index=False, header=False)\n",
        "files.download('10949863-Task1-method-b-validation.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "p9XLPG-34EiZ",
        "outputId": "1d991718-f011-4ee3-ac2d-d6b987d06b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3481758a-fd50-4f12-b12f-67b8d1116cf1\", \"10949863-Task1-method-b-validation.csv\", 3342)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the test dataset"
      ],
      "metadata": {
        "id": "oAr3pBjwri0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_task_1 = pd.read_csv('./data/Task-1-test-dataset1.csv', header=None, names=['id', 'term1', 'term2', 'value'])"
      ],
      "metadata": {
        "id": "GZtvd4dVri7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the cosine similarity for word2vec for test"
      ],
      "metadata": {
        "id": "8ujB4_Bs2Vl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "# Creating a vocabulary for dealing with OOV words\n",
        "vocabulary = set(word2vec_model.wv.key_to_index.keys())\n",
        "print(len(vocabulary))\n",
        "# to store the similarities along with the ids\n",
        "results = []\n",
        "\n",
        "# Extracting adjacent pairs of terms\n",
        "term_pairs = [(term1, term2) for term1, term2 in zip(test_data_task_1['term1'], test_data_task_1['term2'])]\n",
        "\n",
        "def cosine_similarity_word2vec(term1, term2, word2vec_model):\n",
        "    # Checking if the terms are in the vocabulary or not\n",
        "    tokens_1 = term1.split()\n",
        "    tokens_2 = term2.split()\n",
        "\n",
        "    # vectors1 = [word2vec_model.wv.get_vector(token) if token in vocabulary else np.full(word2vec_model.vector_size, 0.01) for token in tokens1]\n",
        "    vectors_1 = []\n",
        "    for token in tokens_1:\n",
        "        if token in vocabulary:\n",
        "            vectors_1.append(word2vec_model.wv.get_vector(token))\n",
        "        else:\n",
        "            vectors_1.append(np.full(word2vec_model.vector_size, 0.01))\n",
        "    # vectors2 = [word2vec_model.wv.get_vector(token) if token in vocabulary else np.full(word2vec_model.vector_size, 0.01) for token in tokens2]\n",
        "    vectors_2 = []\n",
        "    for token in tokens_2:\n",
        "        if token in vocabulary:\n",
        "            vectors_2.append(word2vec_model.wv.get_vector(token))\n",
        "        else:\n",
        "            vectors_2.append(np.full(word2vec_model.vector_size, 0.01))\n",
        "    avg_vector_1 = np.mean(vectors_1, axis=0)\n",
        "    avg_vector_2 = np.mean(vectors_2, axis=0)\n",
        "\n",
        "    avg_vector_1_2d = avg_vector_1.reshape(1, -1)\n",
        "    avg_vector_2_2d = avg_vector_2.reshape(1, -1)\n",
        "\n",
        "    similarity = cosine_similarity(avg_vector_1_2d, avg_vector_2_2d)[0][0]\n",
        "\n",
        "    return similarity\n",
        "\n",
        "# Calculate cosine similarity for each term pair\n",
        "for id, (term1, term2) in zip(range(1, len(test_data_task_1) + 1), term_pairs):\n",
        "    similarity = cosine_similarity_word2vec(term1, term2, word2vec_model)\n",
        "    results.append((id, similarity))\n",
        "\n",
        "# Create a DataFrame from the results without specifying column names\n",
        "results_df = pd.DataFrame(results, columns=['id', 'cosine_similarity'])\n",
        "\n",
        "# Save the DataFrame to a CSV file without including the index and column names\n",
        "results_df.to_csv('10949863-Task1-method-b-test.csv', index=False, header=False)\n",
        "\n",
        "# Download the file to your local machine\n",
        "files.download('10949863-Task1-method-b-test.csv')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WlfvF9wH2VvK",
        "outputId": "80978eb8-eccf-4472-bf1a-3c0cf334aeb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "121631\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9312ddf6-6a95-45ff-a13c-1015bb2eefc5\", \"10949863-Task1-method-b-test.csv\", 2244)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}